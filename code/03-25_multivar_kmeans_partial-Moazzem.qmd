---
title: "Multivariate - K-means"
format: html
---

# Learning objectives  
Our learning objectives are to:  
  - Run k-means for clustering  

You will need to watch (and code along with) the following k-means video:

  - k-means lab: https://youtu.be/GFgMp5tYiMU?si=CI0E-2r-cYZLGVL1 (start from beginning of video, watch till the 01:10)  
  - The partial script for the video above is in our GitHub repository, 04-classcode, **03-25_multivar_kmeans_partial.qmd**. Move the script mentioned above into the `code` subfolder of your `10_multivar` project

# Setup  
```{r}
#| message: false
#| warning: false

#install.packages("ggcorrplot")
#install.packages("factoextra")
#install.packages("ggpmisc")

library(tidyverse)
library(ggcorrplot)
library(broom)
library(car)
library(factoextra)
library(ggpmisc)
```

```{r}
weather 

weather
```

Since k-means only take numerical variables, let's select them now.  
```{r selecting only numerical vars}
weather_n <- weather %>%
  dplyr::select(-c(year:strength_gtex))

weather_n
```

# EDA  
The EDA here is the same as for the PCA script, refer to that if needed.

# Difference between supervised and unsupervised algorithms:

***Supervised algorithm:*** We provide the y variable (aka label in ML) that we are trying to predict to the algorithm. We give the information on y variable that we are trying to predict to the model. E.g., we are trying to predict fiber strength using all 72 weather realated variables. Since fiber strength is the y variable on the formula, this is the supervised algorithm because we are giving the y variable to the algorithm. 

***Unsupervised algorithm:*** We do not provide the y variable (aka label in ML) [Along with k-means, PCA is one example of unsupervised because no y variable was given in PCA]

# k-means  
k-means is an **unsupervised** clustering algorithm and partitions the data into k groups, where k is defined by the user.  

k-means works by  

- randomly choosing k samples from our data to be the initial cluster centers  
- calculates the distance of all observations to the clusters centers  
- assigns a cluster class to each observation based on closest distance  
- using all members of a cluster, recalculates cluster mean  
- repeats the entire process until cluster means stabilize  

```{r k-means in action}
knitr::include_graphics("https://miro.medium.com/max/960/1*KrcZK0xYgTa4qFrVr0fO2w.gif")

```

k-means:  

- Is used for clustering  
- Is an unsupervised analysis (no outcome)  
- Only takes predictors  
- Predictors need to be numerical (no categorical variables)  
- *Does not handle NAs i.e., missing values* 


*k-means is useful when clusters are circular, but can fail badly when clusters have odd shapes or outliers.*    

```{r clustering algorithms comparison}

knitr::include_graphics("https://miro.medium.com/max/1400/1*oNt9G9UpVhtyFLDBwEMf8Q.png")

```

k-means does not normalize our data for us like PCA did, so we will need to do that before running the model.  

```{r}

# normalizing the data
weather_norm  <- weather_n %>%
  mutate(across(everything(), ~scale(.)) )

weather_norm

summary(weather_norm) #Very important note: in the normalized data frame, the mean of all the variables are 0 [i.e., the mean of all variables are standardized around 0]

```

Also, we need to define the number of clusters we want.    

Any thoughts?   

Answer: Based on the "Individuals - PCA Plot" generated in the "PC1 vs PC2" code chunk of "03-25_multivar_pca_partial_Shakir.qmd" code script, we can visualize the possibility of 6 clusters. 

Let's try 2.  

# Very Important Note:

The 1st step of every kmeans run of the algorithm is that it's gonna pull 6 random observations (because we have 6 centers) from our dataset, and have those as starting centers of the clusters, and then it's gonna iterate to find the best solution. But the 1st step was always a random selection of 6 observations. Because it has this randomness component to it, it could happne that we may -- by random chance -- select 6 of the observations that are from the same specific place of the 72 dimensional place of our data, that once kmeans finishes the exercise, it may not find the actual most appropriate center of those clusters. Let's say, If our 6 random cluster centers are all clustered on 1 side of the data distribution, it may be difficult for us to spread them out in a way that makes sense and find all the proper cluster centers. Again, it has the randomness component in the 1st step, and for us to avoid that we get an answer from kmeans that was impacted/affected by the random selection of the initial centers. "nstart" here just makes sure that it runs kmeans 10 times. So, what we saw on the .GIF from step 0 to step 12 was 1 full run. By specifying "nstart = 10", we are asking kmeans to repeat these steps 10 times in a way if some of these 10 full runs gives us a suboptimal center of the clusters, it's still gonna return us the best results from these 10 runs so that we do not fall victim to the randomness side of the 1st step if run 1 time/ 1 full run. In short, by specifying "nstart = 10", it's doing the kmeans steps 10 times, and returning us the best results across those 10 attempts.  

```{r kmeans model }

mod_km <- kmeans(weather_norm,
                 centers = 6,
                 nstart = 10 
                 )

#The 1st step of every kmeans run of the algorithm is that it's gonna pull 6 random observations (because we have 6 centers) from our dataset, and have those as starting centers of the clusters, and then it's gonna iterate to find the best solution. But the 1st step was always a random selection of 6 observations. Because it has this randomness component to it, it could happne that we may -- by random chance -- select 6 of the observations that are from the same specific place of the 72 dimensional place of our data, that once kmeans finishes the exercise, it may not find the actual most appropriate center of those clusters. Let's say, If our 6 random cluster centers are all clustered on 1 side of the data distribution, it may be difficult for us to spread them out in a way that makes sense and find all the proper cluster centers. Again, it has the randomness component in the 1st step, and for us to avoid that we get an answer from kmeans that was impacted/affected by the random selection of the initial centers. "nstart" here just makes sure that it runs kmeans 10 times. So, what we saw on the .GIF from step 0 to step 12 was 1 full run. By specifying "nstart = 10", we are asking kmeans to repeat these steps 10 times in a way if some of these 10 full runs gives us a suboptimal center of the clusters, it's still gonna return us the best results from these 10 runs so that we do not fall victim to the randomness side of the 1st step if run 1 time/ 1 full run. In short, it's doing the kmeans steps 10 times, and returning us the best results across those 10 attempts.

mod_km

```

Since the choice of k can be subjective, we will need to find an objective way to select the value of k that most properly represents our dataset.    

```{r choosing k - total error}

# Total error x k



```


```{r choosing k - silhouette}
# Silhouette width
fviz_nbclust(weather_norm, 
             method = "s",
             k.max = 10,
             FUNcluster = kmeans) 

```

total error: k=3-4  
silhouette: k=4  

Let's go with 4 clusters.  

```{r mod_km4 }
mod_km4 

mod_km4
```

How many observations per cluster?
```{r}

```

Now how can we visually inspect the resutls of k-means?  
We can either  

- add the cluster column to original dataset and explore the distribution of each variable against cluster id, OR  

- use a function that summarises all the original variables into PCs and plots the cluster ids.  

```{r cluster x variable boxplots}
weather %>%
  mutate(cluster = mod_km4$cluster,
         cluster = factor(cluster)) %>%
  pivot_longer(!c(year,site,cluster)) %>%
  ggplot(aes(x = cluster, 
             y = value, 
             color = cluster))+
    geom_boxplot(show.legend = F)+
  facet_wrap(~name, scales = "free_y", ncol = 6)

ggsave("../output/clustervalidation.png",
       width = 10,
       height = 20)  

```
  
We could actually run ANOVA models for each original variable of the form  

              var ~ cluster, 
              for ex. mean_dayl.s_Jan ~ cluster  
  
and extract cluster mean and pairwise comparison to understand what variables had significant differences among clusters.  

```{r kmeans PCA plot}

```
  
Notice how, behind the scenes, the fviz_cluster function ran a PCA and is showing us a plot with PCs 1 and 2 on the axis (same result as we obtained on the PCA analysis).   


# Summary  
In this exercise, we covered:  

  - When multivariate analysis can be used  
  - k-means for clustering  
  - How to validate results from k-means analysis  






  
