---
title: "Multivariate - PCA"
format: html
---

# Learning objectives  
Our learning objectives are to:  

  - Explore what is and when to use multivariate analysis  
  - Explore different types of multivariate analysis  
  - Import our feature-engineered weather data set to be analyzed  
  - Understand what is multicollinearity, its effects, implications, and remedies   
  - Run principal component analysis (PCA) for dimensionality reduction  

# Introduction  
## Terminology  

In the following general equation:  

    y ~ x

Y is normally referred to as:  
  - Response variable [used to refer y in experimental design type of analysis]  
  - Dependent variable [used to refer y in experimental design type of analysis] 
  - Predicted variable  [y variable is referred as "predicted" variable in machine learning]

X is normally referred to as:  
  - Explanatory variable [used to refer x in experimental design type of analysis]  
  - Independent variable [used to refer x in experimental design type of analysis]  
  - Predictor variable  [x variable is referred as "predictor" variable in machine learning]

## What is multivariate analysis?  

Most common: multiple Xs (predictors)  

               y ~ x1 + x2 + .... xn  
               yield ~ precip + temp + som + hybrid  [yield explained by precipitation, temperature, soil organic matter, and hybrid]
    
Perhaps multiple Xs (predictors) without a Y (outcome)  

              ~ x1 + x2 + .... xn

Very Important Note: When we do NOT have any y (predicted) variable, we are not trying to predict something specific (since there are no  y variable), we are just trying to look for pattern on your x (predictor) variables.

While some can have multiple Ys (outcomes)  

               y1 + y2 ~ x1 + x2 (MANOVA) [Multivariate ANOVA Analysis] 
               yield + biomass ~ precip + temp  
  
## When to use multivariate?  
When multiple predictors can be used to explain an outcome.  

When multiple predictors can be used to create data-driven groups (clusters).  

Commonly not a "designed experiment" analysis, but an exploratory relationship approach.  

## Main types of multivariate  
- Dimensionality reduction  
    - **Principal component analysis (PCA)**  [Note: PCA is used when we have many dimensions i.e., a column/variable in our dataframe. For the weather data frame that we previously worked on, we have 72 dimensions/columns/variables that we obtained through feature engineering. For this feature engineered data frame, we can conduct dimensionality reduction technique such as PCA to our dataframe, if we wanted]   
    - Principal coordinate analysis (PCoA)  
    - Factor analysis  
    
- Clustering 
    - **k-means**, c-means, x-means  
    - Non-numerical multidimensional scaling  
    - nearest neighbor  

[Note: the listed clustering algorithms do not take y variable, only takes x variables]
  
- Relationship between variables / Prediction  
  - Multiple linear regression/stepwise regression  
  - **Random forest** [very popular machine learning algorithm that we learn in this class]   
  - **Conditional inference tree** [very popular machine learning algorithm that we learn in this class]   

## A few ways to characterize multivariate analysis  
- Based on analysis goal:  

  - Decrease number of dimensions  
  - Create clusters  
  - Establish significant relationships among multiple predictors and an outcome.  

- Based on Y (outcome) existence:
    - Supervised  
[Note: in supervised, we have the y variable, so we are giving label (= y variable) to the algorithm. It is supervised because the learning of the algorithm is being supervised based on the y variable/label. we give the y variable (aka label) i.e., we have the y variable that we are trying predict]  

    - Unsupervised  
[Note: We do not give/have the y variable (aka label)/ no label. Clustering is unsupervised]

- If Y exists (i.e., supervised learning), based on its type:  
    - Categorical: classification [e.g., low, high level of a y variable (= fiber strength)]    
    - Numerical: regression [y variable is a continuous numerical variable]  

# Setup  
```{r}
#| message: false
#| warning: false

#install.packages("ggcorrplot")
#install.packages("factoextra")
#install.packages("ggpmisc")
#install.packages("ggpmisc")
#installed.packages("ggpubr")


library(tidyverse)
library(ggcorrplot) #helps to look into correlation of variables
library(broom)
library(car)
library(factoextra) #helps to look into some clustering functions
library(ggpmisc) #helps make annotations in ggplots
library(ggpubr) #package to load "stat_cor()" function; "stat_cor()": adds correlation coefficients with p-values to a scatter plot. Can be also used to add 'R2'.
```

```{r}

weather <- read_csv("../../09_opendata/data/weather_monthsum.csv")

weather

```


# EDA  

How are variables related? Any strong correlations that we should watch out for?    

Let's create a correlation matrix  

```{r correlation matrix p-values}

# Estimating significance matrix

p.mat <- weather %>%
  dplyr::select(-year, -site) %>%
  cor_pmat() #to get all potential bivariate correlation #lot of numbers, so difficult to make sense of. So, we are gonna plot it

p.mat

```

Plotting  
```{r correlation matrix plot}
weather %>%
  dplyr::select(-year, -site) %>%
  cor() %>%
  ggcorrplot(hc.order = TRUE, 
             digits = 1,
             type = "lower", 
             p.mat = p.mat, 
             sig.level = 0.05,
             insig = "blank",
             lab = TRUE) #to create matrix heatmap of all correlations

ggsave("../output/corrmat.png",
       height = 25,
       width = 22,
       bg = "white")
```

A bit difficult to see because we have so many predictors, let's extract in table format.  

```{r highest correlations}

weather %>%
  dplyr::select(-year, -site) %>%
  cor() %>% # calculates r (-1 to 1)
  as.data.frame() %>% #to turn into a data frame
  rownames_to_column() %>% #to turn the rownames into a column
  pivot_longer(cols = -rowname) %>%
  filter(abs(value) > .85 & value != 1) %>% #to filter absolute values of correlation coefficients that are higher than .85 and not equal to 1
  arrange(desc(value)) %>% #to arrange in descending order of correlation
  distinct(value, .keep_all = T) #to keep the distinct values of correlation

```

```{r lowest correlations}

weather %>%
  dplyr::select(-year, -site) %>%
  cor() %>% # calculates r (-1 to 1)
  as.data.frame() %>%
  rownames_to_column() %>%
  pivot_longer(cols = -rowname) %>% #pivoting longer everything except "rowname"
  filter(abs(value) < .05 & value != 1) %>% #to filter absolute values of correlation coefficients that are less than .05 and not equal to 1
  arrange(abs(value)) %>%
  distinct(value, .keep_all = T)

```

How do variables relate to fiber strength in a bivariate relationship?  

```{r r2}

test <- weather %>%
  dplyr::select(-site, -year) %>%
  pivot_longer(cols = -strength_gtex) %>% #pivoting longer everything except "strength_gtex"
  group_by(name) %>%
  nest() %>%
  mutate(r2 = map_dbl(data,
                  ~lm(strength_gtex ~ value,
                      data = .x) %>%
                    glance(.) %>% #Extracts model metrics from any model
                    pull(r.squared)
                  )) %>% #map_dbl() return a double
  arrange(desc(r2))


test$data[[1]] %>%
  lm(strength_gtex ~ value,
     data = .) %>%
  glance(.)  %>% 
  pull(r.squared)

```


```{r r2}

weather %>%
  dplyr::select(-site, -year) %>%
  pivot_longer(cols=-strength_gtex) %>%
  group_by(name) %>%
  nest() %>%
  mutate(r2 = map_dbl(data, #"map_dbl()" is another map function from map family of functions (from "purrr" package); map_dbl() returns double, whereas map() returns any object 
                  ~lm(strength_gtex ~ value,
                      data = .x) %>%
                    glance(.) %>% #Extracts model fit metrics from any model
                    pull(r.squared) #just to extract/pull only the value of R2
                  )) %>% #map_dbl() return a double
  arrange(desc(r2))

```

```{r r2 plot}

weather %>%
  dplyr::select(-site, -year) %>%
  pivot_longer(cols=-strength_gtex) %>%
  group_by(name) %>%
  nest() %>%
  mutate(r2 = map_dbl(data,
                  ~lm(strength_gtex ~ value,
                      data = .x) %>%
                    glance(.) %>%
                    pull(r.squared)
                  )) %>%
  arrange(desc(r2)) %>%
  ungroup() %>%
  slice(1:6) %>% #to get only the 6 highest R2 values
  unnest(data) %>%
  ggplot(aes(x = value, 
             y = strength_gtex))+
  geom_point(shape = 21, 
             alpha = .7, 
             fill = "purple")+
  geom_smooth(method = "lm", 
              se = F, 
              color = "black", 
              size = 1)+
  facet_wrap(~name, 
             scales = "free_x", 
             ncol=2) 

```

# Multicollinearity 
## Concepts  
Multicollinearity definition: more than two explanatory variables [i.e., predictor variables] in a multiple regression model are highly linearly related.  


Multicollinearity is an issue because:  

- Model estimates (e.g., slope, intercept) magnitude and direction (+ or -) can change for multicollinear variables compared to a non-multicollinear model.  
  
- Model estimates (e.g., slope, intercept) standard error are inflated, directly affecting p-values, estimate significance, and power.  
  

## Applied example  
Let's select a few variables to run some tests.  
Two uncorrelated variables:  

```{r uncorrelated}

#install.packages("ggpmisc")
#library(ggpmisc)

installed.packages("ggpubr")
library(ggpubr)

weather %>%
  ggplot(aes(x = mean_dayl.s_Apr, 
             y = sum_prcp.mm_Oct)) +
  geom_point() +
  geom_smooth(method="lm") +
  stat_cor() #Adds correlation coefficients with p-values to a scatter plot. Can be also used to add 'R2'.

#?stat_cor()

```

Two correlated variables:  

```{r correlated}

weather %>%
  ggplot(aes(x = mean_dayl.s_Apr, 
             y = mean_dayl.s_May))+
  geom_point() +
  geom_smooth(method="lm") +
  stat_cor()

```

Now let's fit some models with one or two uncorrelated and correlated variables explaining fiberstrength and see what happens.  

```{r strength ~ mean_dayl.s_Apr}

lm_dayl.apr <- lm(strength_gtex ~ mean_dayl.s_Apr, 
                  data = weather
                  ) %>%
  tidy() %>% #gives us the estimates i.e., (Intercept) and the slope (= mean_dayl.s_Apr)
  filter(term != "(Intercept)") %>% #to keep only the slopes of the model
  mutate(mod = "lm_dayl.apr")

lm_dayl.apr

```

```{r strength ~ sum_prcp.mm_Oct}

lm_prp.oct <- lm(strength_gtex ~ sum_prcp.mm_Oct,
   data = weather) %>%
  tidy() %>%
  filter(term!="(Intercept)") %>%
  mutate(mod="lm_prp.oct")

lm_prp.oct

```

```{r strength ~ mean_dayl.s_Apr + sum_prcp.mm_Oct}

lm_dayl.prcp <- lm(strength_gtex ~ mean_dayl.s_Apr + 
                   sum_prcp.mm_Oct,
   data = weather) %>%
  tidy() %>%
  filter(term != "(Intercept)") %>%
  mutate(mod = "lm_2var.uncor")

lm_dayl.prcp

```

```{r strength ~ mean_dayl.s_Apr + mean_dayl.s_May}

lm_daylapr.daylmay <- lm(strength_gtex ~ mean_dayl.s_Apr + 
                  mean_dayl.s_May,
   data = weather) %>%
  tidy() %>%
  filter(term!="(Intercept)") %>%
  mutate(mod="lm_2var.cor") #linear model with 2 variables correlated

lm_daylapr.daylmay

```

```{r Checking multicollinearity}
lm_dayl.apr %>%
  bind_rows(lm_prp.oct, 
            lm_dayl.prcp,
            lm_daylapr.daylmay) %>%
  mutate(mod=factor(mod,
                    levels=c("lm_dayl.apr",
                             "lm_prp.oct",
                             "lm_2var.uncor",
                             "lm_2var.cor"))) %>%
  filter(term != "mean_dayl.s_May") %>%
  ggplot(aes(x=mod)) +
  geom_pointrange(aes(y = estimate,
                      ymin = estimate - std.error,
                      ymax = estimate + std.error
                      )) +
  facet_wrap(~ term, scales = "free_y") + #scales = "free_y": to auto-adjust the scales of y axis
  scale_x_discrete(guide = guide_axis(n.dodge = 2))

```

What has happened with **mean_dayl.s_April** and **sum_prcp.mm_Oct** estimates and standard error when modeled i) alone, or with another uncorrelated variable vs. ii) with another correlated variable?  

One way to check the degree of multicollinearity between two predictos is with the *Variance Inflation Factor (VIF)*.  

VIF values range from 1 to positive infinite.  
General rule of thumb:  

- VIF ~ 1: no multicollinearity  
- VIF between 1 and 5: moderate multicollinearity  
- VIF > 5: severe multicollinearity  

Let's check the variance inflation factor (VIF) of both uncorrelated and correlated models  

```{r vif uncorrelated}

# Uncorrelated
lm(strength_gtex ~ mean_dayl.s_Apr + 
                  sum_prcp.mm_Oct,
   data = weather) %>%
  vif()

```


```{r vif correlated}

# Correlated
lm(strength_gtex ~ mean_dayl.s_Apr + 
                  mean_dayl.s_May,
   data = weather) %>%
  vif()

```

## Dealing with multicollinearity  
So what now?  
How can we deal with correlated variables in a multivariate approach?  
Options:  

- Dimensionality reduction  
    - By hand (selecting predictors based on previous knowledge, literature, etc. - highly recommended)    
    - By math  [e.g., Principal Component Analysis (PCA)]
  
- Algorithm that handles multicollinearity  
    - Variable selection    
    - Multivariate by ensembling multiple bivariates   

Many multivariate approaches deal with some sort of similarity/dissimilarity measure among predictors.  

In those cases, predictors with vastly different scales (e.g. mean_tmin.c_Jan from -10 to 10 C vs mean_dayl.s_Jul from 48000 to 51000) need to be normalized so measurement scale does not affect variable importance.    

Thus, our numerical predictor variables need to be normalized (center and scale) before starting our multivariate analysis.  

Some analysis do the normalization for you (like PCA), and others don't (like k-means), so need to be aware of this to ensure data is normalized.  

Since both PCA and k-means only take numerical variables, let's select them now.  

```{r selecting only numerical vars}

weather_n <- weather %>%
  dplyr::select(-year, -site, -strength_gtex) #Along with year and site, we also want to remove the predictor variable

weather_n

```

# PCA  
Principal component analysis (PCA) is a dimensionality reduction approach that accomodates only numerical variables.  

Finds linear relationships among predictors that can be represented in a lower number of uncorrelated dimensions.  

Works well when at least some predictors are correlated.  

PCA:  

- Is used for dimensionality reduction  
- Is an unsupervised analysis (no outcome i.e., no y variable is used in PCA)  
- Only takes predictors  
- Predictors need to be numerical  

Some PCA properties:  

- The number of PCs calculated is the same as the number of variables in the data set (e.g., in our case, 72).  

- The overall variance explained by each PC is *greatest for PC1* and *decreases as PC number increases (e.g., PC72 will explain the least variance)*.  

- All PCs are **orthogonal** (i.e., independent of each other).  

- The goal is to select a small number of PCs that explain a minimum threshold of the total variance (e.g., 70%). [70% is a subjective number]  

```{r pca in action}
knitr::include_graphics("https://miro.medium.com/v2/resize:fit:1200/1*V4H3Cu8qGr_90WANKSO9BA.gif") #copy paste this link in browser to see the animation
```

Let's run PCA for our weather data.  

```{r pca model}

mod_pca <- prcomp(weather_n, scale. = T) #scale. = T: to do scaling before the analysis

```

## Choosing number of PCs  

Based on scree plot (total variance):  

```{r pca checking number of components}

# Scree plot

fviz_eig(mod_pca,
         addlabels = T)

```
    
PCs 1 and 2 explain ~35% and ~26% (61%) of total variance.
This indicates that original variables were correlated (as we expected).   

If wanted to use enough PCs to explain 70% of total variance, how many would we need?  

```{r PCs to explain 70pct variance}

mod_pca %>%
  get_eig() %>%
  mutate(pc = 1:nrow(.)) %>%
  ggplot(aes(x = pc,
             y = cumulative.variance.percent)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = 70)

```
    
We would need 4 PCs.  

Normally we wish to use 2-3 PCs, but 4 is certainly better than 72 original variables.  

## Inspecting PCs  

Let's inspect PC1 through the loadings (weights) of each variable towards it.  

What are the weights that each variable received in this PC?  

```{r PC1 weights}

mod_pca$rotation %>%
  as.data.frame() %>%
  rownames_to_column(var = "var") %>%
  ggplot(aes(x = reorder(var,desc(PC1)), 
             y = PC1))+
  geom_bar(stat = "identity", 
           aes(fill = PC1), 
           show.legend = F) +
  scale_fill_gradient(low = "red", high = "blue") +
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1))

```

Which variables contributed most to PC1, regardless of direction?  

```{r variables contributing to PC1}

fviz_contrib(mod_pca,
             choice = "var",
             axes = 1
             )

```
  
Let's check the eigenvectors for both PCs 1 and 2 variables:  

```{r pca variable contribution }

fviz_pca_var(mod_pca, 
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE,     # Avoid text overlapping,
             select.var = list(contrib = 5) #What does this line do? Ask Dr. Bastos #used in last year's class code
             )

```
  
The longer is the eigenvector for a given variable, the more important it is towards that PC.  

Let's plot PC1 vs PC2 scores, look for any groupings.  

```{r PC1 vs PC2}

pcaplot <- fviz_pca_ind(mod_pca,
             label = F)

ggsave("../output/PC1vsPC2.png",
       height = 5,
       width = 7,
       bg = "white")

```
  
Appears that some groups exist.  


What did we learn?  

- Some original variables strongly correlated  
- Need at least 4 PCs to cover ~70% of original variables variance  
- Most important variables in PC1 were related to day length in winter and summer months [i.e., related to seasonality].  

What now?  
Let's add the first 4 PCs to our original dataset and run a regression versus grain yield.    

```{r pca scores}

# Extract first 4 PCs scores
pca_scores <- mod_pca$x %>%
  as.data.frame() %>%
  dplyr::select(PC1:PC4)

pca_scores

```

```{r pca regression}

# Adding PCs 1-4 scores to original data set
weather_postpca <- weather %>%
  bind_cols(pca_scores) 

# Regression of yield ~ PCs

lm_pca <- lm(strength_gtex ~ PC1 + PC2 + PC3 + PC4,
             data = weather_postpca
             )

# Summary  
summary(lm_pca)

```


```{r pca regression}

# Checking VIF
vif(lm_pca) #All PC 's are orthogonal or independent PC s which is one of the main advantages of PCA

```

```{r PC1 pca regression plots}

# Plotting strength vs PC1
ggplot(weather_postpca,
       aes(x = PC1,
           y = strength_gtex
           )
       ) +
  geom_point() +
  geom_smooth(method = "lm")

```


```{r PC2 pca regression plots}
# Plotting strength vs PC2
ggplot(weather_postpca, 
       aes(x = PC2, y = strength_gtex))+
  geom_point()+
  geom_smooth(method = "lm")

```

```{r PC3 pca regression plots}

# Plotting strength vs PC3
ggplot(weather_postpca, 
       aes(x = PC3, y = strength_gtex))+
  geom_point()+
  geom_smooth(method = "lm")

```

```{r PC4 pca regression plots}

# Plotting strength vs PC3
ggplot(weather_postpca, 
       aes(x = PC4, y = strength_gtex))+
  geom_point()+
  geom_smooth(method = "lm")

```

Only PCs 2 and 3 explained strength (look at which variables were most important to each PC for interpretation).  

As expected, PCs were not multicollinear (VIF=1).  

Note: Each Principal Component is a linear combination of all the 72 original variables. The number of the Principal Components that we get is equal to the number of original variables, so we have 72 total Principal Components. Always PC1 explains the most variability, and that decreases as the number of PCs increases.

# Summary  
In this exercise, we covered:  

  - When multivariate analysis can be used  
  - How multicollinearity is an issue and what to do to fix it   
  - PCA for dimensionality reduction  
  - How to validate results from PCA   




  
